# ğŸ±ğŸ¶ Image Classification (CNN & Transfer Learning)

This project demonstrates **binary image classification (Cat vs Dog)** using **three different deep learning approaches** in **TensorFlow & Keras**, progressing from a **custom CNN** to **Transfer Learning with VGG16**.

---

## ğŸ“Œ Project Summary

- **Task**: Binary Image Classification (Cat vs Dog)
- **Framework**: TensorFlow & Keras
- **Input Sizes**:
  - Custom CNN: `256 Ã— 256 Ã— 3`
  - VGG16 Models: `150 Ã— 150 Ã— 3`
- **Output**: Sigmoid probability  
  (`0 â†’ Cat`, `1 â†’ Dog`)
- **Loss Function**: Binary Crossentropy

---

## ğŸ“‚ Dataset

dataset/

â”‚â”€â”€ train/

â”‚ â”œâ”€â”€ cats/

â”‚ â””â”€â”€ dogs/

â”‚
â”‚â”€â”€ test/

â”‚ â”œâ”€â”€ cats/

â”‚ â””â”€â”€ dogs/


- **Training Images**: 20,000  
- **Validation Images**: 5,000  

ğŸ“ **Dataset Link**:  
ğŸ‘‰ [https://www.kaggle.com/datasets/princelv84/dogsvscats](https://www.kaggle.com/datasets/princelv84/dogsvscats)

---

## ğŸ§  Models Implemented

### ğŸ”¹ 1. Custom CNN (From Scratch)
- Built using Conv2D, BatchNormalization, MaxPooling & Dense layers
- Trained directly on raw images
- **Validation Accuracy**: ~78â€“80%
- Shows limitations of training deep CNNs from scratch on large images

---

### ğŸ”¹ 2. VGG16 â€“ Feature Extraction
- Pretrained **VGG16 (ImageNet)** used as a frozen feature extractor
- Custom classifier added on top
- **Validation Accuracy**: ~91â€“92%
- Faster convergence and better performance than custom CNN

---

### ğŸ”¹ 3. VGG16 â€“ Fine Tuning
- Upper layers of VGG16 unfrozen (`block5` onwards)
- Lower learning rate + Dropout
- **Validation Accuracy**: ~95%
- Best generalization on unseen images

---

## âš™ï¸ Training Pipeline

- Dataset loaded using `image_dataset_from_directory`
- Batch size: `32`
- Images normalized to `[0,1]`
- Optimizers used:
  - Adam (Custom CNN & Feature Extraction)
  - RMSprop (Fine-Tuning, low LR)

---

## ğŸ§ª Testing on Unseen Images

- Tested on real-world images using OpenCV
- Model successfully predicts unseen cat/dog images

```python
model.predict(image)
# 0 â†’ Cat | 1 â†’ Dog
```
---

## ğŸ“Œ Key Takeaways

- Transfer Learning significantly outperforms training from scratch
- Freezing pretrained layers prevents overfitting
- Fine-tuning improves validation accuracy
- Lower learning rates are crucial during fine-tuning

---

## ğŸš€ How to Run (Google Colab)

- Open notebook in Colab
- Upload dataset ZIP
- Unzip dataset:
```python
!unzip archive.zip
```
- Run all cells sequentially
